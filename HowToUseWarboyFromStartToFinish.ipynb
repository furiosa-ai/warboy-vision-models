{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0155b004",
   "metadata": {},
   "source": [
    "# How to Use Warboy From Start To Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acc516",
   "metadata": {},
   "source": [
    "This notebook demonstrates how to use warboy from start to finish with yolov8n object detection model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88fc89",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33985c6",
   "metadata": {},
   "source": [
    "### Install Driver, Firmware, and Runtime packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff735",
   "metadata": {},
   "source": [
    "First, you can install the Driver, Firmware, and Runtime packages for the NPU device through the APT server. To do this, you need to set up the APT server. You can follow the instructions in [Korean](https://developer.furiosa.ai/docs/latest/ko/software/installation.html) or [English](https://developer.furiosa.ai/docs/latest/en/software/installation.html).\n",
    "\n",
    "After setting up the APT server, you can install the packages using the following command:\n",
    "\n",
    "```console\n",
    "$ sudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\n",
    "```\n",
    "\n",
    "Next, you can check NPU devices on your environment using the following command:\n",
    "\n",
    "```console\n",
    "$ sudo apt-get install -y furiosa-toolkit\n",
    "$ furiosactl info -full\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cae78a",
   "metadata": {},
   "source": [
    "### Install Furiosa Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b7c6e",
   "metadata": {},
   "source": [
    "To install the Furiosa Python SDK, you need Python 3.8 or higher. First, you can create a virtual environment with Conda using the following command:\n",
    "\n",
    "```console\n",
    "$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "$ sh ./Miniconda3-latest-Linux-x86_64.sh\n",
    "$ source ~/.bashrc\n",
    "$ conda create -n furiosa-3.9 python=3.9\n",
    "$ conda activate furiosa-3.9\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8232a",
   "metadata": {},
   "source": [
    "Nest, the Furiosa SDK needs to have been installed. If not, it can be installed following instructions on [Korean](https://furiosa-ai.github.io/docs/latest/ko/) or [English](https://furiosa-ai.github.io/docs/latest/en/).\n",
    "\n",
    "```console\n",
    "$ pip install 'furiosa-sdk[full]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447550f",
   "metadata": {},
   "source": [
    "### Install Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728adc2",
   "metadata": {},
   "source": [
    "In this notebook, we will use the COCO dataset. You can download the COCO dataset using the following command:\n",
    "\n",
    "```console\n",
    "./coco.sh\n",
    "```\n",
    "This will download the COCO dataset and save it in the `datasets/coco` directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "508d6ab4",
   "metadata": {},
   "source": [
    "Also, to run web demo, you need to install the demo videos. You can download the demo videos using the following command:\n",
    "\n",
    "```console\n",
    "./demo_videos.sh\n",
    "```\n",
    "\n",
    "This will download the demo videos and save them in the `datasets/demo_videos` directory. This includes the object detection and instacne segmentation videos in `datasets/demo_videos/detection` and pose estimation videos in `datasets/demo_videos/estimation` directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249448",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bf019",
   "metadata": {},
   "source": [
    "You can install the required packages using the following command:\n",
    "\n",
    "```console\n",
    "$ pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0928a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416e12d",
   "metadata": {},
   "source": [
    "Also, in jupyter notebook, because we cannot use `asyncio.run()`, we need to use `nest_asyncio` to run the async function. You can install `nest_asyncio` using the following command:\n",
    "\n",
    "```console\n",
    "$ pip install nest_asyncio\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49785e57",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install nest_asyncio\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a44f4",
   "metadata": {},
   "source": [
    "### Installing a Custom CLI Tool (Optional)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663bfcd",
   "metadata": {},
   "source": [
    "In this notebook, we won't be using the custom CLI tool, but if you want to use it, you can install our custom CLI tool to run vision models on Warboy using the following command:\n",
    "\n",
    "```console\n",
    "$ pip install .\n",
    "```\n",
    "This will install the `warboy-vision` command line tool, which you can use to run models on Warboy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a89bc51",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2847d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import onnx\n",
    "from typing import List, Dict\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import nest_asyncio\n",
    "import asyncio\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e1344",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27ae0f",
   "metadata": {},
   "source": [
    "First, you need to prepare the weight file for the model you want to use. In this notebook, we will use the YOLOv8n model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8c15e",
   "metadata": {},
   "source": [
    "### Download YOLOv8n Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec5b3c",
   "metadata": {},
   "source": [
    "You can download the YOLOv8n weights using the following command:\n",
    "\n",
    "```console\n",
    "wget https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9bb7c",
   "metadata": {},
   "source": [
    "### Export ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa980d4c",
   "metadata": {},
   "source": [
    "To run the model on Warboy, you need a quantized ONNX model. First, let's export the YOLOv8n model to ONNX format.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17178311",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "def export_onnx(\n",
    "    weight_file: str =\"yolov8n.pt\", \n",
    "    input_shape: List[int] = [1, 3, 640, 640],\n",
    "    onnx_path: str =\"models/onnx/object_detection/yolov8n.onnx\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Export YOLOv8 model to ONNX format.\n",
    "    Args:\n",
    "        weight_file (str): Path to the YOLOv8 model weights file.\n",
    "        input_shape (tuple): Shape of the input tensor.\n",
    "        onnx_path (str): Path to save the exported ONNX model.\n",
    "    \"\"\"\n",
    "\n",
    "    print(f\"Load PyTorch Model from {weight_file}...\")\n",
    "    if os.path.dirname(onnx_path) != \"\" and not os.path.exists(\n",
    "        os.path.dirname(onnx_path)\n",
    "    ):\n",
    "        os.makedirs(os.path.dirname(onnx_path))\n",
    "\n",
    "    # Load the PyTorch model in inference mode\n",
    "    torch_model = YOLO(weight_file).model.eval()\n",
    "\n",
    "    print(f\"Export ONNX {onnx_path}...\")\n",
    "    dummy_input = torch.randn(input_shape).to(torch.device(\"cpu\"))\n",
    "    \n",
    "    torch.onnx.export(\n",
    "        torch_model,\n",
    "        dummy_input,\n",
    "        onnx_path,\n",
    "        opset_version=13,\n",
    "        input_names=[\"images\"],\n",
    "        output_names=[\"outputs\"],\n",
    "    )\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31eb7f2",
   "metadata": {},
   "source": [
    "For yolo models, due to a drop in accuracy after quantization caused by the concatenation operator (which combines class results and box results along the channel axis at each anchor), we need to modified the model by removing the decoding part from the model output.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "def edit_onnx(\n",
    "    onnx_path: str = \"models/onnx/object_detection/yolov8n.onnx\", \n",
    "    edit_info: Dict[str, List] = None\n",
    "):\n",
    "    \n",
    "    \"\"\"\n",
    "    Edit the ONNX model to change input and output shapes.\n",
    "    Args:\n",
    "        onnx_path (str): Path to the ONNX model.\n",
    "        edit_info (dict): Not necessary, can be None and will be computed.\n",
    "    \"\"\"\n",
    "    \n",
    "    from onnx.utils import Extractor\n",
    "    from warboy_vision_models.warboy.tools.utils import get_onnx_graph_info\n",
    "\n",
    "    onnx_graph = onnx.load(onnx_path)\n",
    "    input_to_shape, output_to_shape = get_onnx_graph_info(\n",
    "        \"object_detection\", \"yolov8n\", onnx_path, edit_info\n",
    "    )\n",
    "\n",
    "    edited_graph = Extractor(onnx_graph).extract_model(\n",
    "        input_names=list(input_to_shape), output_names=list(output_to_shape)\n",
    "    )\n",
    "\n",
    "    for value_info in edited_graph.graph.input:\n",
    "        del value_info.type.tensor_type.shape.dim[:]\n",
    "        value_info.type.tensor_type.shape.dim.extend(\n",
    "            input_to_shape[value_info.name]\n",
    "        )\n",
    "    for value_info in edited_graph.graph.output:\n",
    "        del value_info.type.tensor_type.shape.dim[:]\n",
    "        value_info.type.tensor_type.shape.dim.extend(\n",
    "            output_to_shape[value_info.name]\n",
    "        )\n",
    "\n",
    "    print(f\"Export edited ONNX >> {onnx_path}\")\n",
    "    onnx.save(edited_graph, onnx_path)\n",
    "    \n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d425ba",
   "metadata": {},
   "source": [
    "Let's run the code below to export and edit the YOLOv8n model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7ad234e",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = \"yolov8n.pt\"\n",
    "onnx_path = \"models/onnx/object_detection/yolov8n.onnx\"\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "export_onnx(\n",
    "    weight_file=weight_file,\n",
    "    input_shape=input_shape,\n",
    "    onnx_path=onnx_path,\n",
    ")\n",
    "\n",
    "edit_onnx(\n",
    "    onnx_path=onnx_path,\n",
    "    edit_info=None,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d702e",
   "metadata": {},
   "source": [
    "### Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911b994",
   "metadata": {},
   "source": [
    "Next, let's quantize the ONNX model. Quantization is a technique that converts a high-precision (usually FP32) DL model to a lower precision, reducing the model size and memory cost, and improving the inference speed. By quantizing the model, you can run efficient inference AI services.\n",
    "\n",
    "You can see the specifics of quantization in [Korean](https://developer.furiosa.ai/docs/latest/ko/software/quantization.html) or [English](https://developer.furiosa.ai/docs/v0.5.0/en/advanced/quantization.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5b12a",
   "metadata": {},
   "source": [
    "In quantization phase, we need to prepare the calibration dataset. The calibration dataset is used to calibrate the quantization parameters of the model. In this notebook, we will use COCO dataset for calibration.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0b1058",
   "metadata": {},
   "source": [
    "We need to preprocess the COCO dataset to use it for calibration. Preprocessor will be also used to preprocess the input data for inference.\n",
    "\n",
    "For YOLO models, we will resize the input image to 640x640. You can check the `YoloPreProcessor` code in `warboy_vision_models/warboy/yolo/preprocess.py` file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1991093",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_calibration_dataset(\n",
    "    calibration_data_path: str = \"datasets/coco/val2017\",\n",
    "    num_calibration_data: int = 100,\n",
    "):\n",
    "    \"\"\"\n",
    "    Get calibration dataset for quantization.\n",
    "    Args:\n",
    "        calibration_data_path (str): Path to the dataset.\n",
    "        num_calibration_data (int): Number of images to use for calibration.\n",
    "    \"\"\"\n",
    "    \n",
    "    import glob\n",
    "    import imghdr\n",
    "    import random\n",
    "\n",
    "    calibration_data = []\n",
    "\n",
    "    datas = glob.glob(calibration_data_path + \"/**\", recursive=True)\n",
    "    datas = random.choices(datas, k=min(num_calibration_data, len(datas)))\n",
    "\n",
    "    for data in datas:\n",
    "        if os.path.isdir(data) or imghdr.what(data) is None:\n",
    "            continue\n",
    "        calibration_data.append(data)\n",
    "    print(calibration_data)\n",
    "    return calibration_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ae286",
   "metadata": {},
   "outputs": [],
   "source": [
    "def quantize(\n",
    "    onnx_path: str = \"models/onnx/object_detection/yolov8n.onnx\",\n",
    "    onnx_i8_path: str = \"models/quantized_onnx/object_detection/yolov8n_i8.onnx\",\n",
    "    calibration_data_path: str = \"datasets/coco\",\n",
    "    calibration_method: str = \"SQNR_ASYM\",\n",
    "    num_calibration_data: int = 100,\n",
    "    use_model_editor: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Qauntize the model using FuriosaAI SDK.\n",
    "\n",
    "    Args:\n",
    "        onnx_path (str): Path to the ONNX model.\n",
    "        onnx_i8_path (str): Path to save the quantized ONNX model.\n",
    "        calibration_method (str): Calibration method for quantization. Can check options by help(CalibrationMethod)\n",
    "        use_model_editor (bool): Whether to use model editor for input type conversion.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(os.path.dirname(onnx_i8_path)):\n",
    "        os.makedirs(os.path.dirname(onnx_i8_path))\n",
    "\n",
    "    if not os.path.exists(onnx_path):\n",
    "        raise FileNotFoundError(f\"{onnx_path} is not found!\")\n",
    "\n",
    "    from furiosa.optimizer import optimize_model\n",
    "    from furiosa.quantizer import (\n",
    "        CalibrationMethod,\n",
    "        Calibrator,\n",
    "        ModelEditor,\n",
    "        TensorType,\n",
    "        get_pure_input_names,\n",
    "        quantize,\n",
    "    )\n",
    "\n",
    "    new_shape = input_shape[2:]\n",
    "    onnx_model = onnx.load(onnx_path)\n",
    "    onnx_model = optimize_model(\n",
    "        model=onnx_model,\n",
    "        opset_version=13,\n",
    "        input_shapes={\"images\": input_shape},\n",
    "    )\n",
    "\n",
    "    calibrator = Calibrator(\n",
    "        onnx_model, CalibrationMethod._member_map_[calibration_method]\n",
    "    )\n",
    "    \n",
    "    from warboy_vision_models.warboy.yolo.preprocess import YoloPreProcessor\n",
    "\n",
    "    preprocessor = YoloPreProcessor(new_shape=new_shape, tensor_type=\"float32\")\n",
    "\n",
    "    for calibration_data in tqdm(\n",
    "        get_calibration_dataset(\n",
    "            calibration_data_path=calibration_data_path, \n",
    "            num_calibration_data=num_calibration_data\n",
    "        ), desc=\"calibration...\"\n",
    "    ):\n",
    "        input_img = cv2.imread(calibration_data)\n",
    "        input_, _ = preprocessor(input_img)\n",
    "        calibrator.collect_data([[input_]])\n",
    "\n",
    "    if use_model_editor:\n",
    "        editor = ModelEditor(onnx_model)\n",
    "        input_names = get_pure_input_names(onnx_model)\n",
    "\n",
    "        for input_name in input_names:\n",
    "            editor.convert_input_type(input_name, TensorType.UINT8)\n",
    "\n",
    "    calib_range = calibrator.compute_range()\n",
    "    quantized_model = quantize(onnx_model, calib_range)\n",
    "\n",
    "    with open(onnx_i8_path, \"wb\") as f:\n",
    "        f.write(bytes(quantized_model))\n",
    "\n",
    "    print(f\"Quantization completed >> {onnx_i8_path}\")\n",
    "    \n",
    "    return\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0bcb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"models/onnx/object_detection/yolov8n.onnx\"\n",
    "onnx_i8_path = \"models/quantized_onnx/object_detection/yolov8n_i8.onnx\"\n",
    "calibration_data_path = \"datasets/coco/val2017\"\n",
    "calibration_method = \"SQNR_ASYM\"\n",
    "num_calibration_data = 1\n",
    "use_model_editor = True\n",
    "\n",
    "quantize(\n",
    "    onnx_path=onnx_path,\n",
    "    onnx_i8_path=onnx_i8_path,\n",
    "    calibration_data_path=calibration_data_path,\n",
    "    calibration_method=calibration_method,\n",
    "    num_calibration_data=num_calibration_data,\n",
    "    use_model_editor=use_model_editor\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594ba99",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5f2c6",
   "metadata": {},
   "source": [
    "We will run inference on the YOLOv8n model with the COCO dataset and then, test the performance. We will use MSCOCODataLoader to load the COCO dataset. The MSCOCODataLoader code is in `warboy_vision_models/test/utils.py` code. We will use the `COCOeval` module from `pycocotools.cocoeval` to evaluate the model performance.\n",
    "\n",
    "We need preprocessor that we created in the previous step to preprocess the input data for inference. The preprocessor will resize the input image to 640x640. We also need postprocessor to postprocess the output data. The postprocessor will decode the output data and draw or calculate the bounding boxes. You can check the `ObjDetPostprocess` code in `warboy_vision_models/warboy/yolo/postprocess.py` file and you can check the `object_detection_anchor_decoder` code in `warboy_vision_models/warboy/yolo/decoder.py` file. And we also need to use `xyxy2xywh` function to convert the bounding boxes from xyxy format to xywh format and `YOLO_CATEGORY_TO_COCO_CATEGORY` to convert the category from YOLO format to COCO format. You can check the both functions in `warboy_vision_models/tests/utils.py` file.\n",
    "\n",
    "In this notebook, we will run inference simply, but if you want to optimize the inference speed, please refer to `warboy_vision_models/warboy/utils/process_pipeline.py` code and `warboy_vision_models/warboy/runtime/warboy_runtime.py` code.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "async def warboy_inference(model, data_loader, preprocessor, postprocessor):\n",
    "    from warboy_vision_models.tests.utils import xyxy2xywh, YOLO_CATEGORY_TO_COCO_CATEGORY\n",
    "\n",
    "    async def task(\n",
    "        runner, data_loader, preprocessor, postprocessor, worker_id, worker_num\n",
    "    ):\n",
    "        results = []\n",
    "        for idx, (img_path, annotation) in enumerate(data_loader):\n",
    "            if idx % worker_num != worker_id:\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img0shape = img.shape[:2]\n",
    "            input_, contexts = preprocessor(img)\n",
    "            preds = await runner.run([input_])\n",
    "\n",
    "            outputs = postprocessor(preds, contexts, img0shape)[0]\n",
    "\n",
    "            bboxes = xyxy2xywh(outputs[:, :4])\n",
    "            bboxes[:, :2] -= bboxes[:, 2:] / 2\n",
    "\n",
    "            for output, bbox in zip(outputs, bboxes):\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"image_id\": annotation[\"id\"],\n",
    "                        \"category_id\": YOLO_CATEGORY_TO_COCO_CATEGORY[int(output[5])],\n",
    "                        \"bbox\": [round(x, 3) for x in bbox],\n",
    "                        \"score\": round(output[4], 5),\n",
    "                    }\n",
    "                )\n",
    "        return results\n",
    "\n",
    "    from furiosa.runtime import create_runner\n",
    "\n",
    "    worker_num = 16\n",
    "    async with create_runner(model, worker_num=32, compiler_config={\"use_program_loading\": True}) as runner:\n",
    "        results = await asyncio.gather(\n",
    "            *(\n",
    "                task(\n",
    "                    runner,\n",
    "                    data_loader,\n",
    "                    preprocessor,\n",
    "                    postprocessor,\n",
    "                    idx,\n",
    "                    worker_num,\n",
    "                )\n",
    "                for idx in range(worker_num)\n",
    "            )\n",
    "        )\n",
    "    return sum(results, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91967447",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_yolov8n(\n",
    "    dataset: str = \"datasets/coco/val2017\",\n",
    "    annotation: str = \"datasets/coco/annotations/instances_val2017.json\",\n",
    "    onnx_i8_path: str = \"models/quantized_onnx/object_detection/yolov8n_i8.onnx\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Inference using the quantized ONNX model.\n",
    "\n",
    "    Args:\n",
    "        dataset (str): Path to the dataset.\n",
    "        onnx_i8 (str): Path to the quantized ONNX model.\n",
    "    \"\"\"\n",
    "\n",
    "    from warboy_vision_models.warboy.yolo.preprocess import YoloPreProcessor\n",
    "    from warboy_vision_models.warboy.yolo.anchor_process import object_detection_anchor_decoder\n",
    "    from warboy_vision_models.tests.utils import MSCOCODataLoader\n",
    "    from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "    preprocessor = YoloPreProcessor(\n",
    "        new_shape=input_shape[2:],\n",
    "        tensor_type=\"uint8\"\n",
    "    )\n",
    "    postprocessor = object_detection_anchor_decoder(\n",
    "        model_name=\"yolov8n\",\n",
    "        conf_thres=0.001,   # confidence threshold\n",
    "        iou_thres=0.7,  # NMS IOU threshold\n",
    "        anchors=[None],\n",
    "        use_tracker=False\n",
    "    )\n",
    "\n",
    "\n",
    "    data_loader = MSCOCODataLoader(\n",
    "        Path(dataset),\n",
    "        Path(annotation),\n",
    "        preprocessor,\n",
    "        input_shape,\n",
    "    )\n",
    "\n",
    "    results = asyncio.run(\n",
    "        warboy_inference(onnx_i8_path, data_loader, preprocessor, postprocessor)\n",
    "    )\n",
    "\n",
    "    coco_result = data_loader.coco.loadRes(results)\n",
    "    coco_eval = COCOeval(data_loader.coco, coco_result, \"bbox\")\n",
    "    coco_eval.evaluate()\n",
    "    coco_eval.accumulate()\n",
    "    coco_eval.summarize()\n",
    "\n",
    "    print(\"mAP: \", coco_eval.stats[0])\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d01df347",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "test_yolov8n(\n",
    "    dataset=\"datasets/coco/val2017\",\n",
    "    annotation=\"datasets/coco/annotations/instances_val2017.json\",\n",
    "    onnx_i8_path = \"models/quantized_onnx/object_detection/yolov8n_i8.onnx\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e72197",
   "metadata": {},
   "source": [
    "## NPU Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0e4c5",
   "metadata": {},
   "source": [
    "In Furiosa SDK, we provide a profiling tool to analyze the performance of the model. You can use the profiling tool to measure the time taken by each operation in the model and identify the bottlenecks in the model.\n",
    "\n",
    "After running the command, the trace file will be saved in the `models/trace` directory. You can visualize the trace analysis using the Chrome web browser's Trace Event Profiling Tool (chrome://tracing). This will help you understand the performance of the model and optimize it for better performance.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddb14b",
   "metadata": {},
   "source": [
    "There can be `OpenTelemetry trace error occurred. cannot send span to the batch span processor because the channel is full` warning messages when writing the trace file. But you can ignore them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3180de0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_warboy_performance(input_shape, task, device, model, onnx_i8_path):\n",
    "    from furiosa.runtime.sync import create_runner\n",
    "    from furiosa.runtime.profiler import profile\n",
    "    \n",
    "    input_shape = input_shape\n",
    "    trace_dir = os.path.join(\"models/trace\", task)\n",
    "\n",
    "    if not os.path.exists(trace_dir):\n",
    "        os.makedirs(trace_dir)\n",
    "\n",
    "    trace_file = os.path.join(trace_dir, model + \"_\" + device + \".log\")\n",
    "    dummy_input = np.uint8(np.random.randn(*input_shape))\n",
    "    \n",
    "    with open(trace_file, mode=\"w\") as tracing_file:\n",
    "        with profile(file=tracing_file) as profiler:\n",
    "            with create_runner(\n",
    "                onnx_i8_path, device=device, compiler_config={\"use_program_loading\": True}\n",
    "            ) as runner:\n",
    "                for _ in range(30):\n",
    "                    runner.run([dummy_input])\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 640, 640)\n",
    "task = \"object_detection\"\n",
    "device = \"warboy(1)*1\"\n",
    "model = \"yolov8n\"\n",
    "onnx_i8_path = \"models/quantized_onnx/object_detection/yolov8n_i8.onnx\"\n",
    "\n",
    "test_warboy_performance(\n",
    "    input_shape=input_shape,\n",
    "    task=task,\n",
    "    device=device,\n",
    "    model=model,\n",
    "    onnx_i8_path=onnx_i8_path\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "furiosa-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
