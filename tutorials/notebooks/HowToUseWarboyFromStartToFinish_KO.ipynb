{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0155b004",
   "metadata": {},
   "source": [
    "# How to Use Warboy From Start To Finish"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06acc516",
   "metadata": {},
   "source": [
    "YOLOv8n 모델을 예시로 Warboy를 사용하는 방법을 보여주는 튜토리얼 주피터 노트북입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea88fc89",
   "metadata": {},
   "source": [
    "## Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17260641",
   "metadata": {},
   "source": [
    "### Make Python Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d6b7c6e",
   "metadata": {},
   "source": [
    "이 주피터 노트북의 코드들은 Python 3.9 이상의 환경이 필요합니다. 이미 해당하는 Python 환경이 있다면 이 단계를 건너뛸 수 있습니다. 만약 없다면 Conda를 통해 Python 환경을 새롭게 만들 수 있습니다.\n",
    "\n",
    "Conda가 설치되어 있지 않는 경우, 아래의 명령어를 통해 Miniconda를 설치할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ wget https://repo.anaconda.com/miniconda/Miniconda3-latest-Linux-x86_64.sh\n",
    "$ sh ./Miniconda3-latest-Linux-x86_64.sh\n",
    "$ rm -rf Miniconda3-latest-Linux-x86_64.sh\n",
    "$ source ~/.bashrc\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b39d9105",
   "metadata": {},
   "source": [
    "Miniconda를 설치한 후, 다음의 명령어를 사용해 새로운 Python 3.9 환경을 만들 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ conda create -n furiosa-3.9 python=3.9\n",
    "$ conda activate furiosa-3.9\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33985c6",
   "metadata": {},
   "source": [
    "### Install Driver, Firmware, and Runtime packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3faff735",
   "metadata": {},
   "source": [
    "Warboy를 사용하기 위해서는 가장 먼저, Warboy NPU에 맞는 드라이버, 펌웨어, 런타임 패키지를 설치해야 합니다. 이를 위해서는 우선 APT 서버를 설정해야 하며, 그 방법은 [Korean](https://developer.furiosa.ai/docs/latest/ko/software/installation.html) 또는 [English](https://developer.furiosa.ai/docs/latest/en/software/installation.html)에서 확인할 수 있습니다.\n",
    "\n",
    "APT 서버 설정을 완료한 후, 아래의 명령어를 통해 드라이버, 펌웨어, 런타임 패키지를 설치할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ sudo apt-get update && sudo apt-get install -y furiosa-driver-warboy furiosa-libnux\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceb22003",
   "metadata": {},
   "source": [
    "패키지 설치를 완료한 후, 아래의 명령어를 통해 NPU 장치가 정상적으로 인식되는지 확인할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ sudo apt-get install -y furiosa-toolkit\n",
    "$ furiosactl info --format full\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19cae78a",
   "metadata": {},
   "source": [
    "### Install Furiosa Python SDK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db8232a",
   "metadata": {},
   "source": [
    "Furiosa Python SDK는 pip를 통해 설치할 수 있습니다. SDK에 대한 자세한 내용은 [Korean](https://furiosa-ai.github.io/docs/latest/ko/) 또는 [English](https://furiosa-ai.github.io/docs/latest/en/)에서 확인할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ pip install 'furiosa-sdk[full]'\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447550f",
   "metadata": {},
   "source": [
    "### Install Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2b99217",
   "metadata": {},
   "source": [
    "이미 dataset을 다운로드 했다면 이 단계를 건너뛸 수 있지만, `CHECK` 표시가 되어있는 부분에서 dataset의 경로가 갖고 있는 dataset의 경로와 일치하는지 확인해주세요.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6728adc2",
   "metadata": {},
   "source": [
    "이 튜토리얼에서는 COCO dataset을 사용할 것이며, 아래의 명령어를 통해 다운로드 할 수 있습니다.\n",
    "\n",
    "```console\n",
    "./coco2017.sh\n",
    "```\n",
    "\n",
    "이 명령어를 실행하면 COCO dataset이 `datasets/coco` 경로에 저장됩니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93249448",
   "metadata": {},
   "source": [
    "### Install required packages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7bf019",
   "metadata": {},
   "source": [
    "아래의 명령어를 통해 필수 패키지를 설치할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ pip install -r requirements.txt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b416e12d",
   "metadata": {},
   "source": [
    "또한, 주피터 노트북에서 `asyncio.run()`을 사용하기 위해서는 `nest_asyncio` 패키지를 설치해야 합니다. 아래의 명령어를 통해 설치할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ pip install nest_asyncio\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c07946d",
   "metadata": {},
   "source": [
    "### Build Yolo Decoders"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a344d525",
   "metadata": {},
   "source": [
    "이 프로젝트에는, YOLOv8n 모델을 위한 decoder가 일부분 C++로 구현되어 있습니다. 아래의 명령어를 통해 이를 빌드할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ ./build.sh\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75a44f4",
   "metadata": {},
   "source": [
    "### Install the Project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b663bfcd",
   "metadata": {},
   "source": [
    "이 프로젝트를 모듈로 설치하기 위해서는 아래의 명령어를 사용합니다.\n",
    "\n",
    "```console\n",
    "$ pip install .\n",
    "```\n",
    "\n",
    "이를 통해 이 프로젝트를 설치하게 되면, `warboy-vision` command-line tool을 사용할 수 있게 됩니다. 아래의 명령어를 통해 자세한 사항을 확인할 수 있습니다.\n",
    "\n",
    "```console\n",
    "$ warboy-vision --help\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d90be2",
   "metadata": {},
   "source": [
    "### Import Modules\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2847d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import glob\n",
    "import imghdr\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import nest_asyncio\n",
    "import numpy as np\n",
    "import onnx\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from ultralytics import YOLO\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac4e1344",
   "metadata": {},
   "source": [
    "## Prepare Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f27ae0f",
   "metadata": {},
   "source": [
    "Warboy에서 모델을 실행하기 위해서는 quantized ONNX 모델이 필요합니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b8c15e",
   "metadata": {},
   "source": [
    "### Download YOLOv8n Weights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bec5b3c",
   "metadata": {},
   "source": [
    "가장 먼저, 실행하고자 하는 모델의 weight가 필요합니다. 이 튜토리얼에서는 YOLOv8n 모델을 사용할 것이며, 이미 weight 파일을 다운로드 했다면 이 단계를 건너뛸 수 있습니다.\n",
    "\n",
    "아래의 명령어를 통해 YOLOv8n 모델의 weight를 다운로드 할 수 있습니다.\n",
    "\n",
    "```console\n",
    "wget https://github.com/ultralytics/assets/releases/download/v8.1.0/yolov8n.pt\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d9bb7c",
   "metadata": {},
   "source": [
    "### Export ONNX"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa980d4c",
   "metadata": {},
   "source": [
    "먼저, YOLOv8n 모델을 ONNX 형식으로 export합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17178311",
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_file = \"yolov8n.pt\"  # CHECK you may change this to your own weight file\n",
    "onnx_path = \"../models/onnx/object_detection/yolov8n.onnx\"  # CHECK you may change this to your own path\n",
    "input_shape = (1, 3, 640, 640)\n",
    "\n",
    "print(f\"Load PyTorch Model from {weight_file}...\")\n",
    "if os.path.dirname(onnx_path) != \"\" and not os.path.exists(\n",
    "    os.path.dirname(onnx_path)\n",
    "):\n",
    "    os.makedirs(os.path.dirname(onnx_path))\n",
    "\n",
    "# Load the PyTorch model in inference mode\n",
    "torch_model = YOLO(weight_file).model.eval()\n",
    "\n",
    "print(f\"Export ONNX {onnx_path}...\")\n",
    "dummy_input = torch.randn(input_shape).to(torch.device(\"cpu\"))\n",
    "\n",
    "torch.onnx.export(\n",
    "    torch_model,\n",
    "    dummy_input,\n",
    "    onnx_path,\n",
    "    opset_version=13,\n",
    "    input_names=[\"images\"],\n",
    "    output_names=[\"outputs\"],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f31eb7f2",
   "metadata": {},
   "source": [
    "Quantization을 하기에 앞서, YOLO 모델들의 경우, channel 축으로 진행하는 concat 연산자에서 quantization 이후 정확도가 떨어지는 문제가 발생합니다. 이를 해결하기 위해 모델을 수정해 decoding 부분을 제거하고, 이후 연산은 따로 진행해주어야 합니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47b9be19",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"../models/onnx/object_detection/yolov8n.onnx\"  # CHECK you may change this to your own path\n",
    "\n",
    "from onnx.utils import Extractor\n",
    "from src.warboy.tools.utils import get_onnx_graph_info\n",
    "\n",
    "onnx_graph = onnx.load(onnx_path)\n",
    "input_to_shape, output_to_shape = get_onnx_graph_info(\n",
    "    \"object_detection\", \"yolov8n\", onnx_path, None\n",
    ")\n",
    "\n",
    "edited_graph = Extractor(onnx_graph).extract_model(\n",
    "    input_names=list(input_to_shape), output_names=list(output_to_shape)\n",
    ")\n",
    "\n",
    "for value_info in edited_graph.graph.input:\n",
    "    del value_info.type.tensor_type.shape.dim[:]\n",
    "    value_info.type.tensor_type.shape.dim.extend(\n",
    "        input_to_shape[value_info.name]\n",
    "    )\n",
    "for value_info in edited_graph.graph.output:\n",
    "    del value_info.type.tensor_type.shape.dim[:]\n",
    "    value_info.type.tensor_type.shape.dim.extend(\n",
    "        output_to_shape[value_info.name]\n",
    "    )\n",
    "\n",
    "print(f\"Export edited ONNX >> {onnx_path}\")\n",
    "onnx.save(edited_graph, onnx_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261d702e",
   "metadata": {},
   "source": [
    "### Quantize Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6911b994",
   "metadata": {},
   "source": [
    "ONNX 모델이 준비되었다면, 양자화(quantization)를 진행합니다. 양자화는 높은 정밀도(주로 FP32)를 지닌 딥러닝 모델을 낮은 정밀도(Warboy에서는 INT8)로 변환해 모델 사이즈를 축소하여 메모리 비용을 줄이고, 추론 속도를 향상시키는 기술입니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d5b12a",
   "metadata": {},
   "source": [
    "양자화 단계에서는 calibration을 위한 dataset이 필요합니다. calibraion dataset은 calibration range를 결정하기 위해 사용되며, 이 튜토리얼에서는 COCO dataset을 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1991093",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_data_path = \"../../datasets/coco/val2017\"   # CEHCK you may change this path to your own path\n",
    "num_calibration_data = 100\n",
    "\n",
    "calibration_dataset = []\n",
    "\n",
    "datas = glob.glob(calibration_data_path + \"/**\", recursive=True)\n",
    "datas = random.choices(datas, k=min(num_calibration_data, len(datas)))\n",
    "\n",
    "for data in datas:\n",
    "    if os.path.isdir(data) or imghdr.what(data) is None:\n",
    "        continue\n",
    "    calibration_dataset.append(data)\n",
    "print(calibration_dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "398a22f9",
   "metadata": {},
   "source": [
    "이제 양자화를 진행해봅시다. calibration 방법과 calibraion data의 개수는 변경할 수 있습니다. 양자화 및 calibraion의 자세한 내용은 [Korean](https://developer.furiosa.ai/docs/latest/ko/software/quantization.html) 또는 [English](https://developer.furiosa.ai/docs/v0.5.0/en/advanced/quantization.html) 문서에서 확인할 수 있습니다.\n",
    "\n",
    "YOLO 모델의 경우, input data를 640x640으로 resize해서 사용할 수 있습니다. 이를 위해 전처리를 진행하는데, 관련 코드는 `src/warboy/yolo/preprocess.py` 파일의 `YoloPreProcessor`에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ae286",
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_path = \"../models/onnx/object_detection/yolov8n.onnx\"  # CHECK you may change this to your own path\n",
    "onnx_i8_path = \"../models/quantized_onnx/object_detection/yolov8n_i8.onnx\"  # CHECK you may change this to your own path\n",
    "calibration_method = \"SQNR_ASYM\"\n",
    "use_model_editor = True\n",
    "\n",
    "if not os.path.exists(os.path.dirname(onnx_i8_path)):\n",
    "    os.makedirs(os.path.dirname(onnx_i8_path))\n",
    "\n",
    "if not os.path.exists(onnx_path):\n",
    "    raise FileNotFoundError(f\"{onnx_path} is not found!\")\n",
    "\n",
    "from furiosa.optimizer import optimize_model\n",
    "from furiosa.quantizer import (\n",
    "    CalibrationMethod,\n",
    "    Calibrator,\n",
    "    ModelEditor,\n",
    "    TensorType,\n",
    "    get_pure_input_names,\n",
    "    quantize,\n",
    ")\n",
    "\n",
    "new_shape = input_shape[2:]\n",
    "onnx_model = onnx.load(onnx_path)\n",
    "onnx_model = optimize_model(\n",
    "    model=onnx_model,\n",
    "    opset_version=13,\n",
    "    input_shapes={\"images\": input_shape},\n",
    ")\n",
    "\n",
    "calibrator = Calibrator(\n",
    "    onnx_model, CalibrationMethod._member_map_[calibration_method]\n",
    ")\n",
    "\n",
    "from src.warboy.yolo.preprocess import YoloPreProcessor\n",
    "\n",
    "preprocessor = YoloPreProcessor(new_shape=new_shape, tensor_type=\"float32\")\n",
    "\n",
    "for calibration_data in tqdm(\n",
    "    calibration_dataset, desc=\"calibration...\"\n",
    "):\n",
    "    input_img = cv2.imread(calibration_data)\n",
    "    input_, _ = preprocessor(input_img)\n",
    "    calibrator.collect_data([[input_]])\n",
    "\n",
    "if use_model_editor:\n",
    "    editor = ModelEditor(onnx_model)\n",
    "    input_names = get_pure_input_names(onnx_model)\n",
    "\n",
    "    for input_name in input_names:\n",
    "        editor.convert_input_type(input_name, TensorType.UINT8)\n",
    "\n",
    "calib_range = calibrator.compute_range()\n",
    "quantized_model = quantize(onnx_model, calib_range)\n",
    "\n",
    "with open(onnx_i8_path, \"wb\") as f:\n",
    "    f.write(bytes(quantized_model))\n",
    "\n",
    "print(f\"Quantization completed >> {onnx_i8_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c594ba99",
   "metadata": {},
   "source": [
    "## Run Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f5f2c6",
   "metadata": {},
   "source": [
    "이 튜토리얼에서는 YOLOv8n 모델을 통해 COCO dataset에 대한 inference를 진행한 후, 성능을 평가할 것입니다.\n",
    "\n",
    "다만, 이 튜토리얼에서는 간단하게 inference를 진행하는 방법을 보여주겠지만, 최적화된 추론을 진행하고 싶다면 `src/warboy/utils/process_pipeline.py`와 `src/warboy/runtime/warboy_runtime.py` 파일을 참고해주세요.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7af45a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = \"../../datasets/coco/val2017\" # CHECK you may change this path to your own path\n",
    "annotation = \"../../datasets/coco/annotations/instances_val2017.json\"   # CHECK you may change this path to your own path\n",
    "onnx_i8_path = \"../models/quantized_onnx/object_detection/yolov8n_i8.onnx\"  # CHECK you may change this to your own path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf148a6e",
   "metadata": {},
   "source": [
    "앞서 설명한 것처럼, YOLOv8n 모델은 640x640으로 resize된 이미지를 입력으로 받기 때문에, 앞서 사용한 것과 동일한 YoloPreProcessor를 사용해 전처리를 진행합니다.\n",
    "\n",
    "또한, COCO dataset을 로드하기 위해 `MSCOCODataLoader`를 사용하는데, 관련 코드는 `src/test_scenarios/utils.py`에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3fe7cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warboy.yolo.preprocess import YoloPreProcessor\n",
    "from src.test_scenarios.utils import MSCOCODataLoader\n",
    "\n",
    "preprocessor = YoloPreProcessor(\n",
    "    new_shape=input_shape[2:],\n",
    "    tensor_type=\"uint8\"\n",
    ")\n",
    "\n",
    "data_loader = MSCOCODataLoader(\n",
    "    Path(dataset),\n",
    "    Path(annotation),\n",
    "    preprocessor,\n",
    "    input_shape,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b16f3930",
   "metadata": {},
   "source": [
    "마지막으로, inference output을 후처리하기 위한 postprocessor가 필요합니다. Postprocessor는 inference 결과를 decode해 바운딩 박스를 그리거나 계산합니다.\n",
    "\n",
    "만약 바운딩 박스가 그려진 이미지를 최종적으로 만들고 싶다면, `src/warboy/yolo/postprocess.py` 파일의 `ObjDetPostprocess`를 사용할 수 있습니다.\n",
    "\n",
    "다만, 이 튜토리얼에서는 mAP를 평가할 것이기 때문에 이미지를 그리는 과정은 생략할 수 있습니다. 이에 따라 postprocessor로 `src/warboy/yolo/decoder.py` 파일의 `object_detection_anchor_decoder`를 사용할 것입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979fadef",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.warboy.yolo.anchor_process import object_detection_anchor_decoder\n",
    "\n",
    "postprocessor = object_detection_anchor_decoder(\n",
    "    model_name=\"yolov8n\",\n",
    "    conf_thres=0.001,   # confidence threshold\n",
    "    iou_thres=0.7,  # NMS IOU threshold\n",
    "    anchors=[None],\n",
    "    use_tracker=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606f4f68",
   "metadata": {},
   "source": [
    "`COCOeval` 함수를 이용해 mAP를 평가할 예정이기 때문에, `xyxy2xywh` 함수를 사용해 바운딩 박스를 xyxy 형식에서 xywh 형식으로 변환하고, `YOLO_CATEGORY_TO_COCO_CATEGORY`를 사용해 카테고리를 YOLO 형식에서 COCO 형식으로 변환해야 합니다. 관련 코드는 `src/test_scenarios/utils.py` 파일에서 확인할 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53e2d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.test_scenarios.utils import xyxy2xywh, YOLO_CATEGORY_TO_COCO_CATEGORY\n",
    "\n",
    "async def warboy_inference(model, data_loader, preprocessor, postprocessor):\n",
    "    async def task(\n",
    "        runner, data_loader, preprocessor, postprocessor, worker_id, worker_num\n",
    "    ):\n",
    "        results = []\n",
    "        for idx, (img_path, annotation) in enumerate(data_loader):\n",
    "            if idx % worker_num != worker_id:\n",
    "                continue\n",
    "\n",
    "            img = cv2.imread(str(img_path))\n",
    "            img0shape = img.shape[:2]\n",
    "            input_, contexts = preprocessor(img)\n",
    "            preds = await runner.run([input_])\n",
    "\n",
    "            outputs = postprocessor(preds, contexts, img0shape)[0]\n",
    "\n",
    "            bboxes = xyxy2xywh(outputs[:, :4])\n",
    "            bboxes[:, :2] -= bboxes[:, 2:] / 2\n",
    "\n",
    "            for output, bbox in zip(outputs, bboxes):\n",
    "                results.append(\n",
    "                    {\n",
    "                        \"image_id\": annotation[\"id\"],\n",
    "                        \"category_id\": YOLO_CATEGORY_TO_COCO_CATEGORY[int(output[5])],\n",
    "                        \"bbox\": [round(x, 3) for x in bbox],\n",
    "                        \"score\": round(output[4], 5),\n",
    "                    }\n",
    "                )\n",
    "        return results\n",
    "\n",
    "    from furiosa.runtime import create_runner\n",
    "\n",
    "    worker_num = 16\n",
    "    async with create_runner(model, worker_num=32, compiler_config={\"use_program_loading\": True}) as runner:\n",
    "        results = await asyncio.gather(\n",
    "            *(\n",
    "                task(\n",
    "                    runner,\n",
    "                    data_loader,\n",
    "                    preprocessor,\n",
    "                    postprocessor,\n",
    "                    idx,\n",
    "                    worker_num,\n",
    "                )\n",
    "                for idx in range(worker_num)\n",
    "            )\n",
    "        )\n",
    "    return sum(results, [])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9535b06",
   "metadata": {},
   "source": [
    "이제 모든 준비가 끝났으니 inference를 진행해봅시다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b43ba999",
   "metadata": {},
   "outputs": [],
   "source": [
    "nest_asyncio.apply()\n",
    "\n",
    "results = asyncio.run(\n",
    "    warboy_inference(onnx_i8_path, data_loader, preprocessor, postprocessor)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb67106",
   "metadata": {},
   "source": [
    "최종적으로, 모델의 성능을 평가하기 위해 `pycocotools.cocoeval` 모듈의 `COCOeval` 함수를 사용해 mAP를 평가합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91967447",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pycocotools.cocoeval import COCOeval\n",
    "\n",
    "coco_result = data_loader.coco.loadRes(results)\n",
    "coco_eval = COCOeval(data_loader.coco, coco_result, \"bbox\")\n",
    "coco_eval.evaluate()\n",
    "coco_eval.accumulate()\n",
    "coco_eval.summarize()\n",
    "\n",
    "print(\"mAP: \", coco_eval.stats[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e72197",
   "metadata": {},
   "source": [
    "## NPU Profiling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d0e4c5",
   "metadata": {},
   "source": [
    "Furiosa SDK에서는 모델의 NPU 성능을 분석하기 위한 프로파일링 도구를 제공합니다. 이 도구를 사용하여 모델의 각 연산에 소요되는 시간을 측정하고, bottleneck이 발생하는 지점을 찾아낼 수도 있습니다.\n",
    "\n",
    "아래의 코드를 실행하면, `tutorials/models/trace` 디렉토리에 trace 파일이 저장됩니다. 이 파일은 Chrome 웹 브라우저의 Trace Event Profiling Tool (chrome://tracing)을 사용하여 시각화할 수 있습니다. 이를 통해 모델의 성능을 이해하고 더 나아가 필요에 따라 최적화까지 진행해볼 수 있습니다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0ddb14b",
   "metadata": {},
   "source": [
    "`OpenTelemetry trace error occurred. cannot send span to the batch span processor because the channel is full` 라는 경고 메시지가 발생할 수 있지만, 무시해도 괜찮은 경고입니다.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de24d98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (1, 3, 640, 640)\n",
    "task = \"object_detection\"\n",
    "device = \"warboy(1)*1\"\n",
    "model = \"yolov8n\"\n",
    "onnx_i8_path = \"../models/quantized_onnx/object_detection/yolov8n_i8.onnx\"  # CHECK you may change this to your own path\n",
    "\n",
    "from furiosa.runtime.sync import create_runner\n",
    "from furiosa.runtime.profiler import profile\n",
    "\n",
    "input_shape = input_shape\n",
    "trace_dir = os.path.join(\n",
    "    \"../models/trace\", task\n",
    ")  # CHECK you may change this to your own path\n",
    "\n",
    "if not os.path.exists(trace_dir):\n",
    "    os.makedirs(trace_dir)\n",
    "\n",
    "trace_file = os.path.join(trace_dir, model + \"_\" + device + \".log\")\n",
    "dummy_input = np.uint8(np.random.randn(*input_shape))\n",
    "\n",
    "with open(trace_file, mode=\"w\") as tracing_file:\n",
    "    with profile(file=tracing_file) as profiler:\n",
    "        with create_runner(\n",
    "            onnx_i8_path, device=device, compiler_config={\"use_program_loading\": True}\n",
    "        ) as runner:\n",
    "            for _ in range(30):\n",
    "                runner.run([dummy_input])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "furiosa-3.9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
